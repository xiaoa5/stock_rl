# stock_rl
为了在金融领域盈利，传统研究方法就是做出一个模型，无论是基本面分析模型（现金流分析等），还是技术面分析（k线形态分析等），通过这个模型可以计算未来的状态，这样人们就可以由此做出决策，也许就可以实现盈利了。但是这个几乎是越来越不可能完成的任务，许多金融数据，比如股市，期货，越来越趋向于随机走势，很难，也许就不可能做出正确的预判。在小范围内或许可以做出成功率比较满意的预判，比如在一个明确的趋势段中，预测似乎很容易。在一个很长的时间段，包含大量数据，包含大量区间波动的这堆数据中，想要得出一个通用的准确率高的模型，这几乎不能实现，传统分析手段也就是基于回归的方法有个最基本的假设就是，以前的数据是可以反应未来的走向的，但是，用长时间段海量数据使用监督式学习的方法去训练这么一个很复杂的神经网络模型几乎也是做不到的，如果数据量很小也许会得到满意的结果，但是这很可能是神经网络只是记住了所有的数据，不是学习到了真实的内容，或者说这就是过渡拟合了。由此可见，强大的非线性工具都不能解决的问题，想要通过相对简单的阿尔法，贝塔这类分析来完成任务这也是不现实的，这不由让人们怀疑那些传统方法的有效性，极端些的揣测就是其实机构不管用什么方法，哪怕什么方法都不用都可以盈利，只是因为其资金优势，还有就是市场的话语权，怎么分析其实并不重要。
那么是不是对于这个市场就不能做什么全面有效分析了吗？是的，也许就真的这样。不过，想想为什么要做分析？还不是为了做投资决策吗？是不是可以跳过分析这一步，直接生成决策呢？一个新假设：虽然市场不可有效预测，但是还是有这么些人可以在市场中经常获利。那么就直接训练这样的机器人直接根据市场做决策，也就是我们要的强化学习机器人，其中当然使用了很多深度学习的技术。
首先了解下强化学习与传统的策略有什么不同，
传统量化策略开发流程：1，数据处理 -》 2，监督式学习模型（各种回归）-》 3，策略生成(人工总结)-》 4，策略回测及参数优化-》 5，模拟实测 -》 6，实时交易
强化学习策略开发流程：1，数据处理 -》 2，训练强化学习模型 -》 3，模拟实测 -》 4，实时交易
虽然传统分析策略所基于的这个模型其实无法得到，传统金融分析师还是很聪明的，他们可以通过一系列阈值这类操作可以是的预测变得可能，但是怎么选这些阈值就是个学问了，这很多都是靠人工经验完成，接着一个策略模型也就基本生成了，然后就是优化这些参数以及阈值了。这类传统策略生成流程需要基于监督式学习的预测模型，所以是很有问题的，参数及阈值的选定与优化也是问题多多，所以最后得出的结论有多少说服力也要打个问号。
基于强化学习的策略生成可以不使用预测模型（model free RL）,当然也有需要预测模型的强化学习（model based RL），由于考虑到找不到合适的广义模型，这里姑且不用。通过海量数据训练，可以直接生成决策，这就是强化学习最强大的地方。已经有很多例子证明，强化学习可以在不同复杂环境直接生成很高级的策略。
建立一个金融领域的强化学习模型的思路：
1 研究对象是否有存在一个动态模型：  下一个状态 = f（现在状态），对于随机过程，这个f函数是找不到的。
2 对于现在所处的状态我们能得到多少有用的信息可用来重建这个一个状态，大多情况下我们并不能得到足够重建这个真实状态的信息，在市场之中这类信息，包括内幕交易，以及各类间接信息。
3 假如有足够数据还原真实状态，假如下一个状态的确可以由现在的状态推的，也就是有这么一个f函数，我们如何找到这个f，通过什么方法？这个f函数是简单的线性回归，还是需要用到庞大的神经网络？如果我们找到这个f的近似函数g，那么事情会变得很美妙：下一个状态的大致情况 = g（现在得到的数据），但是这也许过于乐观了。
4 如果这个公式找不到：下一个状态的大致情况 = g（现在得到的数据），那么就无法做决策了，不过还是有办法做决策的，可以这样： 决策 = rl（现在得到的数据）
5 如果这么个公式找到了：下一个状态的大致情况 = g（现在得到的数据），更可以这样： 决策 = rl（现在得到的数据），因为有了预测模型g可以加速rl的训练，只用少量数据就可以完成训练任务。
6 建立强化学习所需条件：
  1.一个环境，这样可以和策略生成的动作进行互动
  2.一个深度神经网络（Agent），输入环境生成的当前可观测到的状态（Observation + Reward），输出影响环境的动作
7 通过海量有效数据（基本面，技术面，以及内幕消息等）建立训练环境，不断训练强化学习可以得到让人满意的决策。
